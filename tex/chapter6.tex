\chapter{EXPERIMENTS AND RESULTS}\label{results}

This thesis present a few of the possible performance optimizations on looping
constructs for SIA. These optimizations primarily aimed to reduce the network
operation cost and computing cost. This section describes series of experiments
conducted by varying input parameters such as block size as well as the optimization
parameters such as number of blocks to prefetch and its effects on the performance
of the system.

\section{Environment}
These experiments were carried on HiperGator Computer at UF. Table~\ref{tab:hpg2spec}
describes the specification of HiperGator 2. Table~\ref{tab:hpgcomputespecs}
explains the specifications of HiperGator 2 \textbf{compute} nodes and
table~\ref{tab:hpggpuspecs} explains the specifications of HiperGator 2 \textbf{GPU}
nodes. The table~\ref{tab:hpgconnectspecs} describes the specification for the
node interconnect in HiperGator 2.

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name        & Specifications  \\
    \hline
    Total Cores & 30,000          \\
    Memory      & 120 Terabytes   \\
    Storage     & 1 Petabytes     \\
    Max Speed   & 1,100 Teraflops \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 Spec Sheet}
  \label{tab:hpg2spec}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name                       & Specification     \\
    \hline
    Manufacturer               & Dell Technologies \\
    Processor                  & Intel E5-2698v3   \\
    Base Processor Frequency   & 2.3 GHz           \\
    Sockets                    & 2                 \\
    Cores per socket           & 16                \\
    Thread(s) per core         & 1                 \\
    Memory per node            & 128 Gigabytes     \\
    Memory Frequency           & 2133 MHz DDR4     \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 \textbf{Compute} Node}
  \label{tab:hpgcomputespecs}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name                       & Specification     \\
    \hline
    Manufacturer               & Dell Technologies \\
    Processor                  & Intel E5-2683     \\
    Base Processor Frequency   & 2.0 GHz           \\
    Sockets                    & 2                 \\
    Cores per socket           & 14                \\
    Thread(s) per core         & 1                 \\
    GPU                        & Tesla K80         \\
    Memory per node            & 128 Gigabytes     \\
    Memory Frequency           & 2133 MHz DDR4     \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 \textbf{GPU} Node}
  \label{tab:hpggpuspecs}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name             & Specification                                 \\
    \hline
    Node Connection  & Mellanox 56Gbit/s FDR InfiniBand interconnect \\
    Core Switches    & 100 Gbit/s EDR InfiniBand standard            \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 Node interconnect specification}
  \label{tab:hpgconnectspecs}
\end{table}

\section{Prefetching}
This section presents several experiments conducted to investigate the optimal
parameters and tradeoffs involved in the selection of the parameters.

\subsection{\texttt{hit\_ratio}}
To understand the performance of the prefetching mechanism a new metric is introduced.
Prefetch \texttt{hit\_ratio} is defined as the ratio of the number of times the
SIA runtime did not have to block for a certain data block to be ready and total
number of times the data block is accessed:
\[
  \texttt{hit\_ratio} = \frac{number~of~times~no~blocking~required}{total~number~of~times~data~accessed}
\]
The \texttt{hit\_ratio} represents the number of times prefetching was successful
to hide network transfer cost. In the following experiments \texttt{hit\_ratio}
will be used where appropriate to measure effectiveness of parameters in prefetching.

\subsection{Index Length}
The length of indices is the length of the range of indices involved in the loop.
The length of indices can have high impact on prefetching. To study this relation
between index length and prefetching, \texttt{hit\_ratio} is observed by varying
the range of indices. This is presented in figure~\ref{fig:hitratio}.
\begin{figure}[h]
  \input{results/index_length/hitratio}
  \caption{Index Range Length v/s \texttt{hit\_ratio}}
  \label{fig:hitratio}
\end{figure}

Note that the runtime has to block for data only first time it accesses a block.
Subsequent accesses do not need any blocking since the data is ready. Hence the
\texttt{hit\_ratio} with no prefetching is non zero.

If a index spans only 1 then there is no scope for the runtime to do prefetching.
This is evident from the plot when index length is 1, the \texttt{hit\_ratio} with
prefetching is equal to with no prefetching. As range of index length increased,
prefetching gets working. This can be easily observed from exponential growth in
\texttt{hit\_ratio}. And eventually the curve for \texttt{hit\_ratio} flattens out
after 6 since no significant improvement is achieved by increasing the index range
length.

It is observed that as the runtime requests for multiple blocks for prefetching,
the first request to server takes longer as number of index range increases. This
side effect can be explained using the preceding observation about \texttt{hit\_ratio}.
Since the increase in index range length activates prefetching the first request
to server becomes costlier. This is presented in figure~\ref{fig:p_first_mean}.
\begin{figure}[h]
  \input{results/index_length/p_first_mean}
  \caption{Index Range v/s \texttt{wait\_time\_} per iteration}
  \label{fig:p_first_mean}
\end{figure}

It can be concluded from previous observation that prefetching increases the time
for the first request to server. Thus to compensate for the high cost of first
iteration by offsetting it in subsequent iterations the length of index range to
should be sufficient enough. The mean time taken per iteration is plotted against
the length of index range in figure~\ref{fig:p_np_mean}.

\begin{figure}[h]
  \input{results/index_length/p_np_mean}
  \caption{Index Range v/s \texttt{wait\_time\_} per iteration in Prefetched and no Prefetched Loop}
  \label{fig:p_np_mean}
\end{figure}

The length of index should be around 5 to decrease the \texttt{wait\_time\_}
by factor of 2. The mean \texttt{wait\_time} per iteration with prefetching can
reduce upto 3 times as compared to with no prefetching if the length of index
length is greater than 9.

\subsection{Block Size}
Since the time to transfer block over network is related to size of the block, the
block size affects the first request made during prefetching. Along with the first
request, multiple requests for prefetching subsequent blocks are made. This makes
the \texttt{wait\_time\_} for first call sensitive to block size. This is evident
from the graph plotting Block Size against mean \texttt{wait\_time} for first
iteration in figure~\ref{fig:first_wait_time}.
\begin{figure}[h]
  \input{results/block_size/first_wait_time}
  \caption{Block Size v/s \texttt{wait\_time\_} for first iteration}
  \label{fig:first_wait_time}
\end{figure}

As the block size increases, \texttt{wait\_time\_} for first iteration for loops
with prefetch grows much faster than loops without prefetch. At block of size
greater than 500 elements \texttt{wait\_time\_} for first iteration with prefetch
is almost twice the corresponding \texttt{wait\_time\_} without prefetch.

But once the first request for blocks is made, subsequent iterations are not affected
by the block size as compared to loops in which prefetching is not done, since the
runtime need not block for subsequent blocks. This results in overall reduction
in mean \texttt{wait\_time\_}. This trend is presented in figure~\ref{fig:block_size_avg_wait_time}.
\begin{figure}[h]
  \input{results/block_size/avg_wait_time}
  \caption{Block Size v/s Mean \texttt{wait\_time\_} for Prefetched and No Prefetch Loop}
  \label{fig:block_size_avg_wait_time}
\end{figure}

The mean \texttt{wait\_time\_} grows much slower for loops with prefetch compared
to loops without prefetch.

All of these trends of block size against first and mean \texttt{wait\_time\_} for
loops with and without prefetch are summarized in figure~\ref{fig:block_size_avg_all}
\begin{figure}[h]
  \input{results/block_size/avg_all}
  \caption{Block Size v/s Mean \texttt{wait\_time\_} for Prefetched and No Prefetch Loop}
  \label{fig:block_size_avg_all}
\end{figure}

Although the \texttt{wait\_time\_} for first iteration grows at the highest rate,
prefetching compensates for it and keeps the mean \texttt{wait\_time\_} lower than
without prefetching subsequent blocks.

\subsection{Number of Blocks to Prefetch}
As stated in previous sections, prefetching affects the first request to server
and the size of the block also affects the first request. To observe effect of
number of blocks to prefetch on the initial request, the number of blocks were
varied and plotted against mean \texttt{wait\_time\_} for first request. This is
presented in figure~\ref{fig:look_ahead_first_wait_time}.
\begin{figure}[h]
  \input{results/look_ahead/first_wait_time}
  \caption{Number of Block Prefetched v/s \texttt{wait\_time\_} for first request}
  \label{fig:look_ahead_first_wait_time}
\end{figure}

It is clear from the plot that the mean \texttt{wait\_time\_} for the first request
grows linearly with the number of blocks to prefetch. Thus the number of blocks
to prefetch cannot be set at very high number unless the length of index range is
known to be large.

To determine the effect of number of block to prefetch to prefetching, the number
of blocks to prefetch was varied and is plotted against the mean \texttt{wait\_time\_}.
This plot is presented in figure~\ref{fig:look_ahead_avg_wait_time}.
\begin{figure}[h]
  \input{results/look_ahead/avg_wait_time}
  \caption{Number of Block Prefetched v/s mean \texttt{wait\_time\_}}
  \label{fig:look_ahead_avg_wait_time}
\end{figure}

For the case when the number of block prefetched is 0, which is in the case of no
prefetching the mean \texttt{wait\_time\_} is highest. It drops sharply as the
number of blocks to prefetch increases and then it grows again as increase in number
of blocks to prefetch increases the \texttt{wait\_time\_} for first request to
server.

As the number of blocks to prefetched is increased, more blocks are available for
runtime without blocking. This is presented in figure~\ref{fig:look_ahead_hit_ratio}
by plotting number of blocks to prefetch against \texttt{hit\_ratio}.

\begin{figure}[h]
  \input{results/look_ahead/hit_ratio}
  \caption{Number of Block Prefetched v/s Hit Ratio for first request}
  \label{fig:look_ahead_hit_ratio}
\end{figure}

Hit ratio saturates after hitting a critical amount. There is no much use after
that to increase number of blocks to prefetch. This explains the rise in mean
\texttt{wait\_time\_} as \texttt{wait\_time\_} for first request grows but the
number of blocks available without blocking stays constant.

\section{GPU}
\subsection{Memory Pinning}
\subsubsection{Copy Speed}
\begin{figure}[h]
  \input{results/mempin/block_copy/pin_vs_nopin}
  \caption{Time taken to transfer block to GPU for \textit{pinned} and \textit{non pinned} blocks}
\end{figure}
\subsection{Optimized Transfer}
\begin{figure}[h]
  \input{results/optimized_block_transfer/rccsd_rhf}
  \caption{Optimized v/s Unoptimized Block Tranfers for \texttt{rccsd\_rhf.sialx}}
\end{figure}

\subsection{Memory Pinning Overhead}
\subsubsection{\texttt{alloc}}
\begin{figure}[h]
  \input{results/mempin/overhead/alloc}
  \caption{Pinned and non Pinned memory allocation}
\end{figure}

\subsubsection{\texttt{free}}
\begin{figure}[h]
  \input{results/mempin/overhead/free}
  \caption{Pinned and non Pinned memory de-allocation}
\end{figure}