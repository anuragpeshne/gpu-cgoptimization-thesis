\chapter{RELATED WORK}\label{lit}

We discuss work done by others in using GPU for scientific calculations using
several methods such as directive based GPU programming models, using CUDA in
higher level programming language and use of GPU in previous versions of ACES
and the work done in using prefetching to hide latency in accessing data. We also look
at work done in general idea of prefetching such as nonblocking fetch operation,
techniques used to determine optimal prefetching parameters such as the number of
blocks to prefetch and prefetch cache size.

\section{GPGPU}
This section discusses previous work done on General Purpose computing on Graphics
Processing Units.

\subsection{Directive Based GPU programming}
There are several attempts in the area of directive-based programming models. These
attempts include OpenACC~\cite{openacc}, OpenMP for
accelerator~\cite{openmpforaccelerators}, and less updated attempts including
OpenMPC~\cite{openmpc} and hiCuda~\cite{hicuda}. The approach taken by these models
is to make an implementation GPU ready by placing directives which make
execution of loops parallel. In contrast, SIA exploits coarse-grained
concurrency using super instructions and super numbers or blocks by executing
the super instruction on GPU. These super instructions, in turn, exploit fine-grained
concurrency and can make use of above models to make an existing CPU implementation
GPU ready by inserting suitable directives or have a completely rewritten low-level
CUDA implementation. Thus these models work \textit{with} the previously mentioned
models rather than against them.

\subsection{GPU in Computational Chemistry}
There are implementations of coupled cluster methods in computational chemistry
on GPUs reported~\cite{bhaskar2013}\cite{deprince2011}\cite{maw2011}. In these
implementations, a specific algorithm was selected and then implemented on a
GPU in a highly optimized form. These implementations have hardware specific
optimizations, and are not generic enough to remain effective as new hardware
development takes place.

One of the implementations of contraction operators on a GPU by
Ma~et~al.~\cite{Ma2013} generates CUDA code to directly implement the contractions
by optimizing for the particular order of indices in the contractions. This, as
compared to using permutations and then two-dimensional matrix-matrix multiplication
as implemented in SIA, should achieve better performance for a contraction. These
specialized CUDA contraction implementations were added to NWChem, a computational
chemistry package, and considerable speedups are reported for CCSD(T) calculation
on CPU/GPU hybrid systems. Such optimized operators can be incorporated into SIA
and be provided as built-in super instruction for contraction.

\subsection{GPU Programming in Other High-Level Programming Languages}
Several attempts for support for GPU in general purpose high-level programming
languages have been made. A few examples include PyCUDA~\cite{pycuda2011},
jCUDA~\cite{jcuda2009} and Chapel~\cite{chapelgpu}. These toolkits give an interface
to GPU from high-level programming languages such as Python and Java but
programmer still needs to deal with low-level GPU bookkeeping such as memory
management and defining kernels. As described in section~\ref{relatedworkacesiiigpu},
this is similar to ACESIII, wherein the programmer had to manually transfer memory
between GPU and CPU. However, SIAL is a special purpose language and SIA runtime
has information about block sizes and block operations. GPU implementation for
some of the common block operations can defined into the language itself. With
these implementation and the information about block, the runtime can jump between
GPU and CPU implementation transparently to the domain programmer and takes care
of memory management.

\subsection{GPU in Previous Versions of ACES}\label{relatedworkacesiiigpu}
There has been work done previously to exploit GPU in Super Instruction
Architecture by Jindal~et~al~\cite{Jindal2016} in ACESIII. This implementation
required the programmer to mark a block of code to be executed on GPU using
directives \texttt{gpu\_on} and \texttt{gpu\_off} and manually manage memory
transfer to and from GPU. This puts the burden of managing memory and recognizing
the correct block of code suitable for execution on GPU, on the domain programmer.
If non-suitable blocks of SIAL code is marked for executing on GPU, it can result
in suboptimal use of resource. Executing operations on small blocks on GPU can
actually be slower than executing on CPU due to time spent in memory transfer and
insignificant gain in speed. Thus it is better to execute small blocks on CPU and
larger block on GPU. It can be a difficult decision for the SIAL programmer to decide
if a block is large enough so that benefit of executing it on GPU becomes significant.

\subsection{Optimizing GPU Memory Transfer}
\subsubsection{Host Memory Transfer}
GPU cannot work directly with pageable host memory. One of the key condition for
effectively exploiting GPU is to optimally manage data transfer between host memory
and GPU memory. This is true for any computation running on GPU and not just High
Performance Computing computations. Work done by Fatica~\cite{Fatica:2009:ALC:1513895.1513901},
in accelerating Linpack describes various ways for optimizing memory transfers. This
implementation intercepts calls to DGEMM and DTRSM and executes them simultaneously
on both GPUs and CPU cores. Memory allocated on host is pageable by default and GPU
cannot access it. To achieve faster memory transfer the implementation uses fast
\textit{PCI-E} transfers by page locking or \textit{pinning} the host memory. Using
PCI-E transfer the speed improvement of 5 GB/s from 2 GB/s is reported.
Kaldewey~et~al~\cite{Kaldewey:2012:GJP:2236584.2236592} describes similar technique
to optimize memory transfer using common address space for CPU and GPU called as
Unified Memory Addressing (UVA). This helps with memory transfers in two ways: it
allows GPU to work with memory greater than available physical GPU memory and also relieves
the programmer of the burden of managing two memory spaces. UVA requires that the
host memory is page locked. Using UVA, sustained rate of 6.2 GB/s is reported.
Host side memory page locking is a common technique in both which enables DMA controller
to access main memory at PCI-E speeds. This technique can be used to speed up the
SIA block transfer. This can be built into the automatic memory synchronization
so that it is transparent to the domain programmer.

\subsubsection{Network Transfer}
GPUs are often deployed in a cluster to accelerate computationally intensive
general-purpose tasks. Still, for communication the GPUs need assistance from the CPUs.
This requires intermediate copies from the GPU memory to the host memory. To have
a way around this inefficiency, NVIDIA has partnered with Mellanox to make GPUDirect
RDMA available for InfiniBand clusters. Shainer~et~al~\cite{Shainer2011} introduces
GPUDirect that enables GPUs to transfer data via InfiniBand without the  involvement
of the CPU or buffer copies. Using GPUDirect RDMA, third-party devices such as
network adapters can access the GPU memory buffer directly, over the PCI-E bus.
Potluri~et~al~\cite{6687341} evaluated the GPUDirect RDMA for InfiniBand. They reported
improvement in latency of MPI Send/MPI Recv by 69\% and 32\% for message size of
4Byte and 128KByte and improvement in uni-directional bandwidth achieved using
4KByte and 64KByte messages by 2x and 35\%, respectively.
Although, GPUs are not allocated to the SIA servers, GPUDirect feature can still
be exploited to bypass the worker CPUs and to push the GPU memory buffer to the network
adapter. This can also be used to completely avoid host memory by collecting the
SIA block in GPU memory, operating on it using GPU and sending it back bypassing
CPU. This can help in reducing host and GPU memory transfers.

\section{Prefetching}
Prefetching is an old technique~\cite{anacker68}\cite{Smith1982}\cite{Vanderwiel2000}
used to get data up in memory hierarchy before they are actually needed by the processor
with the aim to hide the data access latency due to the difference in speed to access
data. A variety of approaches, including hardware prefetching, and caching, compiler
techniques, pre-execution and runtime execution, have been implemented in
high-performance computing.

Hardware prefetching techniques including techniques such as transferring separate
cache blocks~\cite{Smith1978} were implemented much before software techniques were
implemented. Porterfield~\cite{Porterfield1989} introduced the idea of software prefetching
using special instruction to preload values into the cache without blocking the
computation. Using this instruction the compiler can inform the cache over 100
cycles before a load is required.

Prefetching is also exploited in the area of disk IO apart from feeding processor cache
from memory. Patterson et al~\cite{Patterson1994} implemented informed prefetching
mechanism for IO intensive application to exploit highly parallel IO systems. The
mechanism depends on disclosure of future access dynamically.

Dahlgren and StenstroÌˆm~\cite{Dahlgren1993} developed an algorithm to determine the
number of memory blocks to prefetch. They proposed
\textit{adaptive sequential prefetching} policy, which allows the number of blocks
to prefetch, K to vary during runtime of the program to reflect spatial locality
shown by the program. K is varied by calculating \textit{prefetch efficiency} metric
which is defined as the ratio of useful prefetched blocks to the total number of
prefetched blocks. If the prefetch efficiency exceeds a threshold then K is
incremented and decremented if it drops below a lower threshold. The value of K
can reach 0, effectively disabling prefetching.

There had been work done by Bhatia et al~\cite{Bhatia2010} to determine the size of
the cache to maximize hit rate in a sequential prefetching scheme. In this work, an
online algorithm is devised which saves the blocks evicted from prefetch cache
into another cache, \textit{evicted cache}. If the incoming request is satisfied
by the evicted cache, according to the algorithm the size of prefetch cache is too
small and it increases it. If the request hits prefetch cache or there is a miss
on both of caches then the algorithm leaves the size unchanged. To determine if
the cache size is too large, the eviction cache is observed and if eviction cache
receives no hits then the size of the cache is decremented.

In SIA, blocks which are needed to be prefetched can be predicted with more accuracy
than memory blocks. Further, the exact sequence in which blocks are needed is also known.
This makes prefetching in SIA free of few of the problems faced in general memory
block prefetching discussed in above section. Similar to the problems faced in general
memory prefetching, it is difficult to predict how many blocks to prefetch. But
once the block is prefetched and computed on, the block can be safely evicted from
memory. The problem of how many blocks to prefetch is related to how much memory
should be allocated for prefetching.
