\chapter{EXPERIMENTS AND RESULTS}\label{results}

This thesis presents a few of the possible performance optimizations on looping
constructs for SIA. These optimizations primarily aimed to reduce the network
operation cost and computing cost. This section describes series of experiments
conducted by varying input parameters such as block size as well as the optimization
parameters such as the number of blocks to prefetch and its effects on the performance
of the system.

\section{Environment}
These experiments were carried on HiperGator Computer at UF. Table~\ref{tab:hpg2spec}
describes the specification of HiperGator 2. Table~\ref{tab:hpgcomputespecs}
explains the specifications of HiperGator 2 \textbf{compute} nodes and
table~\ref{tab:hpggpuspecs} explains the specifications of HiperGator 2 \textbf{GPU}
nodes. The table~\ref{tab:hpgconnectspecs} describes the specification for the
node interconnect in HiperGator 2.

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name        & Specifications  \\
    \hline
    Total Cores & 30,000          \\
    Memory      & 120 Terabytes   \\
    Storage     & 1 Petabytes     \\
    Max Speed   & 1,100 Teraflops \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 Spec Sheet}
  \label{tab:hpg2spec}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name                       & Specification     \\
    \hline
    Manufacturer               & Dell Technologies \\
    Processor                  & Intel E5-2698v3   \\
    Base Processor Frequency   & 2.3 GHz           \\
    Sockets                    & 2                 \\
    Cores per socket           & 16                \\
    Thread(s) per core         & 1                 \\
    Memory per node            & 128 Gigabytes     \\
    Memory Frequency           & 2133 MHz DDR4     \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 \textbf{Compute} Node}
  \label{tab:hpgcomputespecs}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name                       & Specification     \\
    \hline
    Manufacturer               & Dell Technologies \\
    Processor                  & Intel E5-2683     \\
    Base Processor Frequency   & 2.0 GHz           \\
    Sockets                    & 2                 \\
    Cores per socket           & 14                \\
    Thread(s) per core         & 1                 \\
    GPU                        & Tesla K80         \\
    Memory per node            & 128 Gigabytes     \\
    Memory Frequency           & 2133 MHz DDR4     \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 \textbf{GPU} Node}
  \label{tab:hpggpuspecs}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    \hline
    Name             & Specification                                 \\
    \hline
    Node Connection  & Mellanox 56Gbit/s FDR InfiniBand interconnect \\
    Core Switches    & 100 Gbit/s EDR InfiniBand standard            \\
    \hline
  \end{tabular}
  \caption{HiperGator 2 Node interconnect specification}
  \label{tab:hpgconnectspecs}
\end{table}

\section{Prefetching}
This section presents several experiments conducted to investigate the optimal
parameters and tradeoffs involved in the selection of the parameters.

\subsection{\texttt{hit\_ratio}}\label{sec:hit_ratio}
To understand the performance of the prefetching mechanism, a new metric is introduced.
Prefetch \texttt{hit\_ratio} is defined as the ratio of the number of times the
SIA runtime did not have to block for a certain data block to be ready and the total
number of times the data block is accessed:
\[
  \texttt{hit\_ratio} = \frac{number~of~times~no~blocking~required}{total~number~of~times~data~accessed}
\]
The \texttt{hit\_ratio} represents the number of times prefetching was successful
to hide network transfer cost. In the following experiments \texttt{hit\_ratio}
will be used to measure the effectiveness of parameters in prefetching.

\subsection{Index Length}
The length of indices is the length of the range of indices involved in the loop.
The length of indices can have a high impact on prefetching. To study this relation
between index length and prefetching, \texttt{hit\_ratio} is observed by varying
the range of indices. This is presented in figure~\ref{fig:hitratio}.
\begin{figure}[h]
  \input{results/index_length/hitratio}
  \caption{Index Range Length v/s \texttt{hit\_ratio}}
  \label{fig:hitratio}
\end{figure}

Note that the runtime has to block for data only first time it accesses a block.
Subsequent accesses do not need any blocking since the data is ready. Hence, the
\texttt{hit\_ratio} with no prefetching is non zero.

If an index spans only 1 then there is no scope for the runtime to do prefetching.
This is evident from the plot when index length is 1, the \texttt{hit\_ratio} with
prefetching is equal to with no prefetching. As the range of index length increased,
prefetching gets working. This can be easily observed from exponential growth in
\texttt{hit\_ratio}. And eventually, the curve for \texttt{hit\_ratio} flattens out
after 6 since no significant improvement is achieved by increasing the index range
length.

It is observed that as the runtime requests for multiple blocks for prefetching,
the first request to the server takes longer as the number of index range increases. This
side effect can be explained using the preceding observation about \texttt{hit\_ratio}.
Since the increase in index range length activates prefetching, the first request
to the server becomes costlier. This is presented in figure~\ref{fig:p_first_mean}.
\begin{figure}[h]
  \input{results/index_length/p_first_mean}
  \caption{Index Range v/s \texttt{wait\_time\_} per iteration}
  \label{fig:p_first_mean}
\end{figure}

It can be concluded from previous observation that prefetching increases the time
for the first request to the server. Thus to compensate for the high cost of the first
iteration by offsetting it in subsequent iterations the length of the index range to
should be sufficient enough. The mean time taken per iteration is plotted against
the length of index range in figure~\ref{fig:p_np_mean}.

\begin{figure}[h]
  \input{results/index_length/p_np_mean}
  \caption{Index Range v/s \texttt{wait\_time\_} per iteration in Prefetched and no Prefetched Loop}
  \label{fig:p_np_mean}
\end{figure}

The length of index should be around 5 to decrease the \texttt{wait\_time\_}
by a factor of 2. The mean \texttt{wait\_time} per iteration with prefetching can
reduce up to 3 times as compared to with no prefetching, if the length of index
length is greater than 9.

\subsection{Block Size}
Since the time to transfer block over the network is related to the size of the block, the
block size affects the first request made during prefetching. Along with the first
request, multiple requests for prefetching subsequent blocks are made. This makes
the \texttt{wait\_time\_} for first call sensitive to block size. This is evident
from the graph plotting Block Size against mean \texttt{wait\_time} for the first
iteration in figure~\ref{fig:first_wait_time}.
\begin{figure}[h]
  \input{results/block_size/first_wait_time}
  \caption{Block Size v/s \texttt{wait\_time\_} for first iteration}
  \label{fig:first_wait_time}
\end{figure}

As the block size increases, \texttt{wait\_time\_} for the first iteration for loops
with prefetch grows much faster than loops without prefetch. At block of size
greater than 500 elements, \texttt{wait\_time\_} for the first iteration with prefetch
is almost twice the corresponding \texttt{wait\_time\_} without prefetch.

But once the first request for blocks is made, subsequent iterations are not affected
by the block size as compared to loops in which prefetching is not done, since the
runtime need not block for subsequent blocks. This results in overall reduction
in mean \texttt{wait\_time\_}. This trend is presented in figure~\ref{fig:block_size_avg_wait_time}.
\begin{figure}[h]
  \input{results/block_size/avg_wait_time}
  \caption{Block Size v/s Mean \texttt{wait\_time\_} for Prefetched and No Prefetch Loop}
  \label{fig:block_size_avg_wait_time}
\end{figure}

The mean \texttt{wait\_time\_} grows much slower for loops with prefetch compared
to loops without prefetch.

All of these trends of block size against first and mean \texttt{wait\_time\_} for
loops with and without prefetch are summarized in figure~\ref{fig:block_size_avg_all}
\begin{figure}[h]
  \input{results/block_size/avg_all}
  \caption{Block Size v/s Mean \texttt{wait\_time\_} for Prefetched and No Prefetch Loop}
  \label{fig:block_size_avg_all}
\end{figure}

Although the \texttt{wait\_time\_} for the first iteration grows at the highest rate,
prefetching compensates for it and keeps the mean \texttt{wait\_time\_} lower than
without prefetching subsequent blocks.

\subsection{Number of Blocks to Prefetch}
As stated in previous sections, prefetching affects the first request to the server
and the size of the block also affects the first request. To observe the effect of
the number of blocks to prefetch on the initial request, the number of blocks were
varied and plotted against mean \texttt{wait\_time\_} for the first request. This is
presented in figure~\ref{fig:look_ahead_first_wait_time}.
\begin{figure}[h]
  \input{results/look_ahead/first_wait_time}
  \caption{Number of Block Prefetched v/s \texttt{wait\_time\_} for the first request}
  \label{fig:look_ahead_first_wait_time}
\end{figure}

It is clear from the plot that the mean \texttt{wait\_time\_} for the first request
grows linearly with the number of blocks to prefetch. Thus the number of blocks
to prefetch cannot be set at very high number unless the length of index range is
known to be large.

To determine the effect of the number of blocks to prefetch to prefetching, the number
of blocks to prefetch was varied and is plotted against the mean \texttt{wait\_time\_}.
This plot is presented in figure~\ref{fig:look_ahead_avg_wait_time}.
\begin{figure}[h]
  \input{results/look_ahead/avg_wait_time}
  \caption{Number of Block Prefetched v/s mean \texttt{wait\_time\_}}
  \label{fig:look_ahead_avg_wait_time}
\end{figure}

For the case when the number of blocks prefetched is 0, which is in the case of no
prefetching the mean \texttt{wait\_time\_} is highest. It drops sharply as the
number of blocks to prefetch increases and then it grows again with an increase in the number
of blocks to prefetch increases the \texttt{wait\_time\_} for the first request to
the server.

As the number of blocks to prefetched is increased, more blocks are available for
runtime without blocking. This is presented in figure~\ref{fig:look_ahead_hit_ratio}
by plotting number of blocks to prefetch against \texttt{hit\_ratio}.

\begin{figure}[h]
  \input{results/look_ahead/hit_ratio}
  \caption{Number of Block Prefetched v/s Hit Ratio for the first request}
  \label{fig:look_ahead_hit_ratio}
\end{figure}

Hit ratio saturates after hitting a critical amount. After that increase, the number
of blocks to prefetch has no effect. This explains the rise in mean
\texttt{wait\_time\_} as \texttt{wait\_time\_} for the first request grows and the
number of blocks available without blocking stays constant.

\subsection{$C_{12}H_{10}(BP)$ Molecule}\label{sec:bp_molecule}
To study the effect of prefetching on real-world application, a job from ACES4 on
$C_{12}H_{10}$ molecule was executed. The total number of workers were 3 and number
of servers were varied to study its effect on prefetching. This is plotted in
figure~\ref{fig:prefetch_real}.

\begin{figure}[h]
  \input{results/prefetch_real/servers}
  \caption{Effects of varying number of servers on $C_{12}H_{10}$ molecule.}
  \label{fig:prefetch_real}
\end{figure}

With the increase in the number of servers, the overall runtime for programs with
prefetching decreases. This is clear from figure~\ref{fig:prefetch_real}, as the difference
in the runtime of programs without prefetch and prefetch increase. It is worth noticing
that the only couple of programs show a significant increase in difference while one
of the programs shows no difference in prefetching and no prefetching. To study
why this is the case, the total \texttt{wait\_time} for the programs is plotted in
figure~\ref{fig:prefetch_real_barrier}.

In the figure~\ref{fig:prefetch_real_barrier}, along with \texttt{wait\_time},
\texttt{barrier wait\_time} is also plotted for all the programs involved. The
\texttt{wait\_time} of program \texttt{scf\_rhf} is mostly due to \texttt{barrier wait\_time}
and rest two have considerably less \texttt{barrier wait\_time} as compared to
overall \texttt{wait\_time}. This explains why only two of the 3 programs showed
significant improvement by prefetching since prefetching helps in reducing \texttt{wait\_time}
for the block to be transferred over the network. The program \texttt{sch\_rhf} did not
have much \texttt{wait\_time} for network transfer.

\begin{figure}[h]
  \input{results/prefetch_real/barrier_wait_time}
  \tikz{\path[draw,fill={rgb:red,97;green,156;blue,45}] (0,0) rectangle (1cm,0.5cm);} w/o Prefetch Total \texttt{wait\_time}.
  \tikz{\path[draw,fill={rgb:red,254;green,56;blue,34}] (0,0) rectangle (1cm,0.5cm);} w/o Prefetch Barrier \texttt{wait\_time}
  \\
  \tikz{\path[draw,fill={rgb:red,30;green,185;blue,3}] (0,0) rectangle (1cm,0.5cm);} w/~~Prefetch Total \texttt{wait\_time}.
  \tikz{\path[draw,fill={rgb:red,195;green,3;blue,3}] (0,0) rectangle (1cm,0.5cm);}  w/~~Prefetch Barrier \texttt{wait\_time}
  \caption{\texttt{wait\_time} and barrier \texttt{wait\_time} variation againsts number of servers.}
  \label{fig:prefetch_real_barrier}
\end{figure}
\section{GPU}
This section presents the experiments performed to benchmark the performance of
using Graphics Processing Unit.
\subsection{Memory Pinning}
Many of the optimization of involves page locking the host memory so that GPU can
bypass processor and access the memory directly. This section presents various
operations which exploit page locked memory.
\subsubsection{Copy Speed}
GPU can access only page-locked memory. Without explicit page-locked memory, the
GPU driver first copies the memory to a temporary page locked memory and then copies
to GPU buffer. By explicitly page locking memory, saving one copy operation is expected.
Figure~\ref{fig:mempin_block_copy} presents varying block size against time to
copy block.

\begin{figure}[h]
  \input{results/mempin/block_copy/pin_vs_nopin}
  \caption{Time taken to transfer block to GPU for \textit{pinned} and \textit{non pinned} blocks}
  \label{fig:mempin_block_copy}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{l | l | l}
    \hline
    Block Size & Pinned      & Not Pinned  \\
    \hline
    225        & 6.50641e-05 & 5.64773e-05 \\
    400        & 5.69895e-05 & 4.88665e-05 \\
    625        & 5.77066e-05 & 4.97829e-05 \\
    900        & 7.04844e-05 & 5.80885e-05 \\
    1225       & 6.93239e-05 & 5.98077e-05 \\
    1600       & 7.10636e-05 & 8.65366e-05 \\
    2025       & 7.18348e-05 & 6.23930e-05 \\
    2500       & 7.40327e-05 & 6.16256e-05 \\
    3025       & 7.27288e-05 & 6.21453e-05 \\
    3600       & 7.76853e-05 & 6.55819e-05 \\
    50625      & 5.90589e-05 & 0.000120046 \\
    160000     & 0.000131335 & 0.000248844 \\
    390625     & 0.000305546 & 0.000510937 \\
    810000     & 0.000607461 & 0.001069850 \\
    1500625    & 0.001107430 & 0.002165080 \\
    2560000    & 0.001884320 & 0.003161620 \\
    4100625    & 0.003069470 & 0.004814120 \\
    6250000    & 0.004606550 & 0.006790370 \\
    9150625    & 0.006778680 & 0.010549700 \\
    12960000   & 0.009527120 & 0.015224000 \\
    \hline
  \end{tabular}
  \caption{Block size against time taken to copy data in page locked and non-page locked memory}
  \label{tab:mempin_block_copy}
\end{table}

It is evident from the plot and table~\ref{tab:mempin_block_copy} that explicit
page locking only results in faster memory copy if the size of the block is above
1600 elements. After the size of block crosses a threshold, the pinned blocks are
copied almost twice as fast as compared to non-pinned blocks.

\subsection{Optimized Transfer}
The block transfer optimization considers whether the block is requested with
intention of being read or written. Hence, to benchmark this optimization, real-world
test case is needed. Here, \texttt{rccsd\_rhf.sialx} is used to investigate the
number of transfers saved by exploiting the intent. This is presented in
figure~\ref{fig:optimized_block_transfer}.

\begin{figure}[h]
  \input{results/optimized_block_transfer/rccsd_rhf}
  \caption{Optimized v/s Unoptimized Block Transfers for \texttt{rccsd\_rhf.sialx}}
  \label{fig:optimized_block_transfer}
\end{figure}

\subsection{Memory Pinning Overhead}
To benchmark the page lock memory allocation against non-page locked memory allocation
such as allocated using \texttt{malloc} function call, two tests were done. The
first test benchmarks allocation and second benchmarks the deallocation. These two
operations are presented in figure~\ref{fig:mempin_alloc} and figure~\ref{fig:mempin_free}
by plotting time taken by page locked operations against non-page locked operations.

\subsubsection{\texttt{alloc}}
Table~\ref{tab:mempin_alloc} describes the time taken for page locked memory allocation
and non-page locked memory allocation.
\begin{table}[h]
  \centering
  \begin{tabular}{l | l | l}
    \hline
    Block Size & Pinned Memory Alloc & Non Pinned Memory Alloc \\
    \hline
    225        & 3.59733e-05         & 9.35048e-07             \\
    400        & 1.99117e-05         & 4.13507e-07             \\
    625        & 2.71294e-05         & 2.93367e-06             \\
    900        & 1.76281e-05         & 5.90459e-07             \\
    1225       & 2.44398e-05         & 6.18398e-07             \\
    1600       & 1.55177e-05         & 2.54251e-06             \\
    2025       & 2.82563e-05         & 3.18140e-06             \\
    2500       & 2.04109e-05         & 3.00072e-06             \\
    3025       & 1.91461e-05         & 3.29316e-06             \\
    \hline
  \end{tabular}
  \caption{Block Size against Page locked and non-page locked memory allocation}
  \label{tab:mempin_alloc}
\end{table}

\begin{figure}[h]
  \input{results/mempin/overhead/alloc}
  \caption{Pinned and non Pinned memory allocation}
  \label{fig:mempin_alloc}
\end{figure}

It is clear from the table~\ref{tab:mempin_alloc} and the figure~\ref{fig:mempin_alloc}
that the time required to allocate both page locked and non-page locked memory
is independent of the size of memory requested. But the allocating page locked
memory can be around 10 to 100 times costlier than non-page locked memory.

\subsubsection{\texttt{free}}
A similar test was performed to benchmark page locked and non-page locked memory
deallocation. Table~\ref{tab:mempin_free} describes the time taken by the deallocation
operation against the varying size of the block.

\begin{table}[h]
  \centering
  \begin{tabular}{l | l | l}
    \hline
    Block Size & Pinned Memory Free & Non Pinned Memory Free \\
    \hline
    225        & 2.78205e-05        & 4.19095e-07            \\
    400        & 2.54586e-05        & 4.04194e-07            \\
    625        & 2.88431e-05        & 5.94184e-07            \\
    900        & 3.09199e-05        & 4.95464e-07            \\
    1225       & 2.51215e-05        & 2.44007e-07            \\
    1600       & 2.58479e-05        & 2.98023e-07            \\
    2025       & 2.80552e-05        & 3.98606e-07            \\
    2500       & 3.52804e-05        & 6.81728e-07            \\
    3025       & 2.70978e-05        & 2.64496e-07            \\
    \hline
  \end{tabular}
  \caption{Block size against Page locked and non-page locked memory deallocation}
  \label{tab:mempin_free}
\end{table}

\begin{figure}[h]
  \input{results/mempin/overhead/free}
  \caption{Pinned and non Pinned memory de-allocation}
  \label{fig:mempin_free}
\end{figure}

The results are similar to page locked and non-page locked allocation, the time
taken for page locked and non-page locked memory deallocation is independent of
the size of the block. Page locked memory deallocation is around 100 times costlier
than non-page locked memory deallocation.

\subsection{RDMA}
To benchmark RDMA, CUDA aware implementation of MPI was used. The size of the block
was varied against CPU and GPU buffer passed to MPI transfer functions. The results
for \texttt{GET} operation are presented in figure~\ref{fig:mempin_rdma_get}.

\subsubsection{\texttt{GET}}
\begin{figure}[h]
  \input{results/mempin/rdma/get}
  \caption{GPU and CPU buffer passed to \texttt{GET}}
  \label{fig:mempin_rdma_get}
\end{figure}


The highest amount of time is taken by passing main memory address to MPI function,
followed by passing page locked main memory address to MPI and passing GPU memory
address to CUDA Aware MPI implementation. Here, there are two important things to
notice:
\begin{enumerate}
\item If CUDA aware MPI implementation is used, then DMA is used for transferring
  data from network fabric to GPU buffers directly.
\item It seems that just by page locking main memory, even without using GPU, DMA
  is invoked by MPI implementation for transfer of data from network fabric to
  main memory.
\end{enumerate}

\subsubsection{\texttt{PUT}}
There is not much of difference in time in \texttt{PUT} operation, as presented
in figure~\ref{fig:mempin_rdma_put}. The time taken by passing main memory address,
page locked main memory address and GPU memory address to MPI function varies slightly
as the size of block increases.

\begin{figure}[h]
  \input{results/mempin/rdma/put}
  \caption{GPU and CPU buffer passed to \texttt{PUT}}
  \label{fig:mempin_rdma_put}
\end{figure}

\subsubsection{Total Transfer}
Even though there is not much improvement in speed if the address of GPU memory is
passed to MPI function, by passing GPU address directly the complete operation
can be executed on GPU without requiring the data to be copied to main memory. This
saves two GPU memory and main memory copy operations and thus get a substantial improvement
in speed of overall operation. The time spent on overall operation is plotted in
figure~\ref{fig:mempin_rdma_total}.

\begin{figure}[h]
  \input{results/mempin/rdma/total_transfer}
  \caption{Total MPI transfer compared to CUDA Aware MPI transfer}
  \label{fig:mempin_rdma_total}
\end{figure}

There is significant speedup as block size grows beyond 2000 with CUDA Aware
implementation getting around 5 times faster than original main memory implementation
for block sizes of around 3000.

\subsection{Caching Page Locked Blocks}
Caching page locked memory blocks is important since allocating page locked memory
is 100 times costlier than non-page locked memory. To test caching, two parameters
have been selected as most appropriate: hit ratio and time spent in allocation with
and without caching.

\textit{Hit Ratio} is simply the ratio of number of times allocation request was
fulfilled by cached block and the total number times memory allocation request
was made.
\[
  \texttt{hit\_ratio} = \frac{number~of~times~memory~allocation~request~fulfilled~by~cached~block}{total~number~of~memory~allocation~requests~made}
\]

Since the Hit Ratio does not depend on the size of memory block requested for, rather
than depends on the pattern of allocation and deallocation, the hit ratio is calculated
for real quantum chemistry calculation program. Table~\ref{tab:mempin_hitrate}
presents the hit ratio for given calculation programs.

\begin{table}[h]
  \centering
  \begin{tabular}{l | r | r | r}
    \hline
    File            & Hit   & Miss & Ratio      \\
    \hline
    scf\_rhf\_coreh &  7975 &  214 & 0.97386738 \\
    tran\_rhf\_no4v &  8028 &  229 & 0.97226596 \\
    rccsd\_rhf      & 12859 & 1435 & 0.89960823 \\
    rccsdpt\_aaa    & 14899 & 3012 & 0.83183519 \\
    rccsdpt\_aab    & 15822 & 3696 & 0.81063634 \\
    \hline
  \end{tabular}
  \caption{Page Locked Memory Blocks hit rate}
  \label{tab:mempin_hitrate}
\end{table}

It is evident that cost of allocation of page locked memory is
paid for only less than 20\% of the time for these programs.

To study the effect of caching on actual allocation time, the time taken by actual
allocation in the above-mentioned programs is calculated for the cases when blocks
are cached and not cached. Time taken when page locking is not done altogether is
also calculated to study the cost paid for page locking even after caching with
respect to not page locking. The results are plotted in figure~\ref{fig:mempin_caching}.

\begin{figure}[h]
  \input{results/mempin/caching/caching}
  \caption{Page Locked Cached v/s Page Locked Uncached v/s Non-Page Locked allocation
  and deallocation times}
  \label{fig:mempin_caching}
\end{figure}

By caching the page locked memory blocks, the allocation time is brought down by
almost factor of 10. However, this is still greater than allocation time spent in
case of non-page locked memory. This difference is difficult to plot, the numerical
values of allocation time are presented in table~\ref{tab:mempin_caching}.

\begin{table}[h]
  \centering
  \begin{tabular}{l | r | r | r}
    \hline
    File            & Not Cached & Cached     & Unpinned    \\
    \hline
    scf\_rhf\_coreh & 0.1717890  & 0.00710591 & 0.00138408  \\
    tran\_rhf\_no4v & 0.0119096  & 0.00223255 & 3.35677e-05 \\
    rccsd\_rhf      & 0.7167320  & 0.07038730 & 0.00620503  \\
    rccsdpt\_aaa    & 0.0136935  & 0.00248791 & 8.10297e-05 \\
    rccsdpt\_aab    & 0.0198253  & 0.00259629 & 4.97848e-05 \\
    \hline
  \end{tabular}
  \caption{Page Locked Cached v/s Page Locked Uncached v/s Non-Page Locked allocation
  and deallocation times}
  \label{tab:mempin_caching}
\end{table}

The caching improves the time spent in allocation by a factor of 10. However the time
spent in non-page locked memory allocation is still less than caching by a factor
of 10 and in some cases around 100.