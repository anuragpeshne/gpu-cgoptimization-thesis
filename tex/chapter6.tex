\chapter{EXPERIMENTS AND RESULTS}\label{results}

This thesis present a few of the possible performance optimizations on looping
constructs for SIA. These optimizations primarily aimed to reduce the network
operation cost and computing cost. This section describes series of experiments
conducted by varying input parameters such as block size as well as the optimization
parameters such as number of blocks to prefetch and its effects on the performance
of the system.

\section{Environment}
These experiments were carried on HiperGator Computer at UF. Table~\ref{hpg2spec}
describes the specification of HiperGator 2. Table~\ref{hpgcomputespecs}
explains the specifications of HiperGator 2 compute nodes and table~\ref{hpggpuspecs}
explains the specifications of HiperGator 2 GPU nodes.
\begin{table}[h]
  \centering
  \begin{tabular}{l | c}
    Name        & Specifications  \\ \hline
    Total Cores & 30,000          \\
    Memory      & 120 Terabytes   \\
    Storage     & 1 Petabytes     \\
    Max Speed   & 1,100 Teraflops \\
  \end{tabular}
  \caption{HiperGator 2 Spec Sheet}
  \label{tab:hpg2spec}
\end{table}

\begin{table}[h]
  \begin{tabular}{l | c}
    Name                       & Specification     \\ \hline
    Manufacturer               & Dell Technologies \\
    Processor                  & Intel E5-2698v3   \\
    Base Processor Frequency   & 2.3 GHz           \\
    Sockets                    & 2                 \\
    Cores per socket           & 16                \\
    Thread(s) per core         & 1                 \\
    Memory per node            & 128 Gigabytes     \\
    Memory Frequency           & 2133 MHz DDR4     \\
  \end{tabular}
  \caption{HiperGator 2 Compute Node~\cite{hpg2-spec-sheet}\cite{intel-compute-cpu-spec}}
  \label{tab:hpgcomputespecs}
\end{table}

\begin{table}[h]
  \begin{tabular}{l | c}
    Name                       & Specification     \\ \hline
    Manufacturer               & Dell Technologies \\
    Processor                  & INTEL E5-2683     \\
    Base Processor Frequency   & 2.0 GHz           \\
    Sockets                    & 2                 \\
    Cores per socket           & 14                \\
    Thread(s) per core         & 1                 \\
    GPU                        & Tesla K80         \\
    Memory per node            & 128 Gigabytes     \\
    Memory Frequency           & 2133 MHz DDR4     \\
  \end{tabular}
  \caption{HiperGator 2 GPU Node}
  \label{tab:hpggpuspecs}
\end{table}

\begin{table}[h]
  \begin{tabular}{l | c}
    Name             & Specification   \\ \hline
    Node Connection  & Mellanox 56Gbit/s FDR InfiniBand interconnect \\
    Core Switches    & 100 Gbit/s EDR InfiniBand standard            \\
  \end{tabular}
  \caption{HiperGator 2 Node interconnect specificatioN}
  \label{tab:hpgconnectspecs}
\end{table}

\section{Prefetching}
\subsection{Index Length}
As range of Index increased, prefetching got working. This can be easily observed
from \texttt{hit\_ratio}.
\begin{figure}[h]
  \input{results/index_length/hitratio}
  \caption{Index Range v/s \texttt{hit\_ratio}}
\end{figure}

It is observed that the first request to server during which request for multiple
blocks is made takes longer as number of index range increases.
\begin{figure}[h]
  \input{results/index_length/p_first_mean}
  \caption{Index Range v/s \texttt{wait\_time\_} per iteration}
\end{figure}

Thus to compensate for the high cost of first iteration, it needs index of length
around 5 to decrease the \texttt{wait\_time\_} by factor of 2. The mean \texttt{wait\_time\_}
per iteration decreased as compared to loops with no prefetching.
\begin{figure}[h]
  \input{results/index_length/p_np_mean}
  \caption{Index Range v/s \texttt{wait\_time\_} per iteration in Prefetched and no Prefetched Loop}
\end{figure}

\subsection{Block Size}
Block size affects the first request made during prefetching. Since along with this
request, multiple requests for prefetching are made. This evident from Block Size
v/s mean \texttt{wait\_time} for first iteration.
\begin{figure}[h]
  \input{results/block_size/first_wait_time}
  \caption{Block Size v/s \texttt{wait\_time\_} for first iteration}
\end{figure}

But once the request for blocks are made, subsequent iterations are affected less
by the block size as compared to loops in which prefetching is not done. This results
in overall reduction in mean \texttt{wait\_time\_}.
\begin{figure}[h]
  \input{results/block_size/avg_wait_time}
  \caption{Block Size v/s Mean \texttt{wait\_time\_} for Prefetched and No Prefetch Loop}
\end{figure}

This is can viewed in better way by looking at both mean \texttt{wait\_time\_} and
mean \texttt{wait\_time\_} for first iteration together.
\begin{figure}[h]
  \input{results/block_size/avg_all}
  \caption{Block Size v/s Mean \texttt{wait\_time\_} for Prefetched and No Prefetch Loop}
\end{figure}

\subsection{Number of Blocks to Prefetch}
The number of blocks to prefetch affects the initial request made.
\begin{figure}[h]
  \input{results/look_ahead/first_wait_time}
  \caption{Number of Block Prefetched v/s \texttt{wait\_time\_} for first request}
\end{figure}

To determine optimal number of block to prefetch we need to consider how much
number of block prefetching affect mean \texttt{wait\_time\_}.
\begin{figure}[h]
  \input{results/look_ahead/avg_wait_time}
  \caption{Number of Block Prefetched v/s mean \texttt{wait\_time\_}}
\end{figure}

Hit ratio saturates after hitting a critical amount. There is no much use after
that to increase number of blocks to prefetch.
\begin{figure}[h]
  \input{results/look_ahead/hit_ratio}
  \caption{Number of Block Prefetched v/s Hit Ratio for first request}
\end{figure}

\section{GPU}
\subsection{Memory Pinning}
\subsubsection{Copy Speed}
\begin{figure}[h]
  \input{results/mempin/block_copy/pin_vs_nopin}
  \caption{Time taken to transfer block to GPU for \textit{pinned} and \textit{non pinned} blocks}
\end{figure}
\subsection{Optimized Transfer}
\begin{figure}[h]
  \input{results/optimized_block_transfer/rccsd_rhf}
  \caption{Optimized v/s Unoptimized Block Tranfers for \texttt{rccsd\_rhf.sialx}}
\end{figure}

\subsection{Memory Pinning Overhead}
\subsubsection{\texttt{alloc}}
\begin{figure}[h]
  \input{results/mempin/overhead/alloc}
  \caption{Pinned and non Pinned memory allocation}
\end{figure}

\subsubsection{\texttt{free}}
\begin{figure}[h]
  \input{results/mempin/overhead/free}
  \caption{Pinned and non Pinned memory de-allocation}
\end{figure}