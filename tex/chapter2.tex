\chapter{BACKGROUND: SUPER INSTRUCTION ARCHITECTURE} \label{background}

This chapter introduces the SIA, ACES4, block-based
programming, the design of workers and servers, SIAL, parallel looping constructs
and design of GPU implementation.

\section{SIA}
The SIA is a special purpose, domain agnostic, parallel programming framework which
is engineered for solving computation on very large, possibly sparse, multidimensional
arrays. Programmer using SIA expresses the computation in terms of blocks of multidimensional arrays
and instructions which operate on these blocks.

In contrast, in conventional programming languages a computation is described in terms of individual floating point
number and operations which work on these individual numbers. Aggregating
individual numbers into blocks results in better performance and higher utilization
of resources. However, this adds considerable complexity to programs since now
the programmer has to deal with tedious and error prone index arithmetic.

Algorithms in SIA can be directly expressed in terms of
blocks. This gives the performance benefits of using blocks and at the same time relieves
programmer from error-prone work of dealing with indices and looping. Expressing
computations in terms of blocks has multiple performance advantages: moving data
blocks is more efficient since it has less overhead per individual number
and runtime can overlap computation and network operation since transferring these
blocks will take significant time, resulting in better overall utilization of resources.
There is an added advantage of expressing computation in terms of blocks in a parallel
framework: the work can be distributed among participating nodes in a natural
way. This is essential when multidimensional arrays are too large to hold in
physical memory of a single processor.

\section{Overview of ACES}
ACES4 is an implementation of the SIA for chemical computation. It has been executed
on a variety of architectures but is specially optimized to enable
calculations on leadership-class supercomputers. Using ACES4, computational chemists
try to find approximate solutions to the electronic SchroÌˆdinger equation using Coupled cluster
methods. There exist other methods to calculate approximate solutions but Coupled
cluster methods, although computationally expensive, are one of the most accurate methods.
The chemical computation done using ACES4 uses data of high dimensions. A typical calculation in this domain
takes as input the geometry of a molecular system and a choice of single
particle orbitals as the basis to expand the many-electron quantum-mechanical wave
function. The complex algorithms, which produce properties of the molecular
system, can easily require arrays of double precision floating point upto
several hundred Gigabytes. Of these arrays, at least three need rapid access and
are usually stored in RAM, the rest that are used less frequently can be stored
on disk. The SIA architecture is very suitable for these kinds of calculations since
the workers can work on parts of array concurrently and the servers can swap out
less frequently used arrays to disk.

\section{Architecture of SIA}\label{siaarch}
The SIA can execute instructions in parallel over multiple processors. It can be
deployed and scaled on multiple nodes in a high performance computing cluster
using MPI for internode communication.

The SIA supports arrays of size greater than the combined memory of all processors
involved in computation by providing the facility of storing the chunks which are
not \textit{hot}, that is the chunks which are not going to be used soon, on
larger, although slower, memory on hard drives. This swapping of blocks is automatic
and transparent to the programmer. To facilitate the movement of data among
processing units and swapping out blocks to hard drives, SIA divides available
processors into two groups: workers and servers --- the workers are responsible for
actual execution of instructions on the blocks, while the servers make sure blocks
are served to and from the workers.

\section{SIAL}
SIAL is a programming language provided for expressing problems of the target
domain. The idea behind SIAL is to keep the domain issues separate from
the platform issues. SIAL programs are written by the domain experts
whereas the intricacies involved in execution of SIAL programs, like distribution of
data, parallel execution, memory management, runtime optimizations, are handled
by computing experts.

SIAL, in addition to providing programmers with conventional constructs such as
conditional constructs, looping constructs, procedures, way to import other SIAL
programs like general purpose programming language, also provides special parallel looping construct and a way
to define domain-specific block operations or \textit{super instructions}. It
has several common block operations built in. The
parallel looping construct, \texttt{pardo}, loops over multiple indices and
distributes loop iterations to different processors. This construct is of special
interest to us since the optimizations done using GPUs are mostly done in the
interpretation of this looping construct.

As mentioned above, domain experts can write their own domain-specific
super instructions which take in single or multiple blocks and output a resulting block. These
instructions can be written in any general purpose programming language by following
the SIAL calling interface. There are implementations of super instructions in
C++ and Fortran in ACES4. Since these languages are much
closer to the hardware, these super instructions can be written in a very
optimized way. Further more, this facility can be exploited to port the super instructions to
other computing devices such as GPU by writing these super instructions using
Nvidia CUDA.

\subsection{SIAL Interpreter}
The SIA includes a SIAL compiler which translates human-readable SIAL text to
machine friendly bytecode. This bytecode is interpreted by SIAL interpreter.
Since this interpretation happens at runtime, the interpreter is able to optimize the
execution based on resources available at runtime. If interpreter finds GPU
accessible, it may execute some part of the SIAL program on GPU and if it
doesn't find it, then it can automatically fallback to CPU. Similarly, there are
various optimizations implemented which depend on the amount of physical memory
present on the processor.

\section{Block Structure}
As described above, instead of processing data as a single floating point
number, the SIA processes blocks of floating point numbers. These blocks are chunks
of even larger multidimensional arrays. Blocks are represented inside SIAL
interpreter using the \texttt{Block} class. The SIA supports heterogeneous computing
using other computing devices such as GPUs. Since GPUs have their own device memory
which is separate from main processor memory, there is a facility in the
\texttt{Block} class to represent the block memory in other
computing devices. Along with member attributes which represent block metadata
and member functions which act on the block, the Block class has pointers to
the memory location on each computation unit: CPU, GPU and support for more
computing device such as Intel Xeon Phi.

The \texttt{Block} class, depending upon the active computing device, will return the
appropriate device memory address. There is also logic build into the \texttt{Block}
class to automatically synchronize memory for various devices, so that if one
device modifies the block and then in next instruction, another device wants
to read the block, the block memory will be automatically synchronized and the
next device will read updated memory. This is done by maintaining version numbers
for each memory and then updating the memory based upon the version numbers when
it is accessed.

\section{Executing Super Instructions on GPU}
There are two ways in which GPUs are exploited in the SIA to obtain high concurrency.
First, the super instructions can be written in CUDA.
Using CUDA, these instructions can make use of low-level hardware
features and domain knowledge to fine-tune the implementation. Secondly, some of
the general purpose block operations such as matrix multiplication, addition,
scaling, tensor contraction for GPU are builtin the interpreter
itself. These operations can be imported from highly optimized libraries such as
Nvidia CuBLAS.

The SIAL interpreter takes care of executing GPU or CPU implementation of an
operation based upon availability of implementations and other factors such as
the size of input and output.