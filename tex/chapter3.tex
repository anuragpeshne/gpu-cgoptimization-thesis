\chapter{BACKGROUND: SUPER INSTRUCTION ARCHITECTURE} \label{background}

This chapter introduces the SIA, ACES4, an implementation of SIA, block based
programming, the design of worker and server, SIAL, parallel looping constructs
and design of GPU implementation.

\section{SIA}
The Super Instruction Architecture is a special purpose, domain agnostic,
parallel programming framework which is engineered for solving very large
computation expressed in terms of multi-dimensional arrays and super
instructions to operate on them. Since these multi-dimensional arrays can be too
large to hold in physical memory of a single processor, they are broken into
blocks or super numbers. These super numbers are input to special instructions
written to operate on blocks instead of floating point numbers and hence called
as super instructions.

SIA can execute instructions on the blocks in parallel on multiple processor. To
facilitate movement of blocks among parallel execution units, server-client
architecture is used. SIA provides SIAL, a block oriented domain specific
language (DSL), which can be used to formulate problems of any domain and a way
to write super instructions which are domain specific in an optimized way. The
following sections describe details of working of server client architecture,
SIAL and constructs in it.

\section{Architecture of SIA}
SIA can execute instructions in parallel over multiple processors. It can be
deployed and scaled on multiple nodes in a high performance computing cluster
using MPI for inter process communication. Since the
multidimensional arrays involved in the calculations can be extremely large for
memory of a single processor, SIA divides the arrays into chunks of
manageable size. These chunks can be distributed to different processors and the
processors can work on the chunks concurrently.

SIA supports arrays of size greater than combined memory of all processors
involved in computation by providing facility of storing the chunks which are
not \textit{hot}, that is the chunks which are not going to be used soon, on
larger, although slower, memory on hard drives. This swapping of blocks is automatic
and transparent to the programmer. To facilitate the movement of data among
processing units and swapping out blocks to hard drives, SIA divides available
processors into two groups, workers and servers --- workers are responsible for
actual execution of instructions on the blocks, while servers make sure blocks
are served to and from workers. The division of number of servers versus number
of workers is chosen by SIA but can be overridden by passing command line
arguments.

\section{SIAL}
SIAL is a programming language provided for expressing problems of target
domain. The idea behind SIAL is to keep the domain problem separate from
platform and computing problem. SIAL programs are written by the domain experts
whereas the intricacies involved in execution of SIAL programs, like distribution of
data, parallel execution, memory management, runtime optimizations, are handled
by computing experts.

SIAL, apart from providing programmers with conventional constructs such as
conditional constructs, looping constructs, procedures, way to import other SIAL
programs like general purpose programming language, also provides special parallel looping construct and a way
to define domain specific block operation or \textit{super instruction}. The
parallel looping construct, \texttt{pardo}, loops over multiple indices, and
distributes blocks to difference processors. This construct is of special
interest to us since the optimizations done using GPU are mostly done in the
interpretation of this looping construct.

As mentioned above, domain experts can write their own domain specific
instructions which take in single or multiple blocks and output a resulting block. These
instructions can be written in C, C++ or Fortran. Since these languages are much
more closer to hardware, these super instructions can be written in a very
optimized way. Further, this facility can be exploited to port the super instructions to
other computing devices such as GPU by writing these super instructions in
Nvidia CUDA.

\subsection{SIAL Interpreter}
SIA consists of SIAL compiler which translates human readable SIAL text to
machine friendly byte code. This byte code is interpreted by SIAL interpreter.
Since this interpretation happen at runtime, interpreter is able to optimize the
execution based on resources available at runtime. If interpreter finds GPU
accessible, then it may execute some part of the SIAL program on GPU and if it
doesn't find it then it can automatically fallback to CPU. Similarly there are
various optimizations implemented which depends on amount of physical memory
present on the processor.

\section{Block Structure}
As described above, instead of processing data as a single floating point
number, SIA processes blocks of floating point numbers. These blocks are chunks
of even larger multidimensional arrays. This is represented inside SIAL
interpreter using \texttt{Block} class. Since SIA supports heterogeneous computing
using other computing devices such as GPU and GPU has their own device memory
which is separate from main processor memory, there is a facility in
\texttt{Block} class to represent the block memory in other
computing device. Along with member attributes which represent block metadata
and member functions which act on the block, the Block class has pointers to
memory location on each computation unit: CPU, GPU and support for more
computing device such as Intel Xeon Phi.

The Block class depending upon the active computing device will return the
appropriate device memory address. There is also a logic build into the \texttt{Block}
class to automatically synchronize memory for various devices so that if one
device edits the block and then in next instruction another device wants
to read the block, the block memory will be automatically synchronized and the
next device will read updated memory. This is done by maintaining version numbers
for each memory and then updating the memory based upon the version numbers when
memory is accessed.

\section{Executing Super Instructions on GPU}
There are two ways in which GPU are exploited in SIA to obtain high concurrency.
First the super instructions can be written in close to device hardware Cuda
programming language. These instructions can make use of low level hardware
features and domain knowledge to fine tune the implementation. Secondly some of
the general purpose block operations such as matrix multiplication, addition,
scaling, tensor contraction can be implemented for GPU in the interpreter
itself. These operations can be included from highly optimized libraries such as
Nvidia CuBLAS.

The SIAL interpreter takes care of executing GPU or CPU implementation of an
operation based upon availability of implementations and other factors such as
size of input and output.

\section{Overview of the ACES}
ACES4 is an implementation of SIA for chemical computation. It has been executed
on a variety of architectures, but is especially optimized to enable
calculations on leadership class supercomputers. The chemical computation done
using ACES4 uses data of high dimension. A typical calculation in this domain
takes as input the geometry of a molecular system and a choice of single
particle orbitals as basis to expand the many-electron quantum-mechanical wave
function. The complex algorithms which produce properties of the molecular
system can easily require arrays of double precision floating point of size
several hundred Gigabytes. Of these arrays, at least three need rapid access and
are usually stored in RAM, the rest that are used less frequently can be stored
on disk.