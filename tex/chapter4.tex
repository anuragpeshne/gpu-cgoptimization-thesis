\chapter{BLOCK PREFETCHING}\label{block prefetching}

This chapter presents the idea of prefetching data block from a server to hide the
network transfer time.

\section{Background}\label{prefetchingbackground}
To process a large array, which cannot fit into the memory of a single node, a typical
workflow in SIAL consists of requesting a block of an array from a server in a
\texttt{pardo} looping construct by each participating worker. After processing
it, the resulting block is sent back to a server. This common pattern can be
summarized as single or multiple network bound operation surrounding one or more
compute bound operations.

%\hline
\begin{algorithm}  {Processing large array}
%\hline %
\singlespacing

\begin{algorithmic}[1]\label{alg:SIALWorkFLow}
%\caption{An example of typical workflow in SIAL}
\Loop\Comment{SIAL do/pardo loop}
\State $GET\ A[i, j]$\Comment{Request data from a server asynchronously}
\State $GET\ B[j, k]$\Comment{Network bound}
\State $t\_result[i, k] \gets A[i, j] \times B[j, k]$\Comment{Wait for A, B to be ready}
\State $s\_result \gets A[i, j] \times A[i, j]$\Comment{No need to wait for A}
\State $CALL\ compute\_fun(t\_result[j, k])$\Comment{Compute bound}
\State $PUT\ AB[i, k] \gets t\_result[i, k]$\Comment{Network bound}
\EndLoop
%\hline
\end{algorithmic}
\end{algorithm}

It is clear from the pseudocode that the computing resources are wasted while
waiting for the data to be ready. To improve the wait time of the compute
operation, non-blocking MPI call \texttt{MPI\_Irecv} was exploited to prefetch
the blocks from servers over the network.

\section{Implementation of Prefetching}

\subsection{pardo Loop Implementation}
To determine which block should be prefetched, the runtime needs to predict
the next block in the loop. This depends on what type of loop is being executed.
While the \texttt{do} loop which iterates over the indices one by one in SIAL is
simple, there are multiple implementations of \texttt{pardo} loop, which differ
in the distribution of indices and thus the distribution of blocks over workers:
\begin{itemize}
\item \texttt{SequentialPardoLoop}: it behaves similar to a simple do loop,
  except this loop, can loop over multiple indices.
\item \texttt{StaticTaskAllocParallelPardoLoop}: the indices for this loop are
  determined statically by distributing the block over workers in a cyclic fashion.
\item \texttt{BalancedTaskAllocParallelPardoLoop}: to support symmetric arrays,
  SIAL has \texttt{where} construct in loops which prunes the iteration based on
  some programmer-defined condition. Due to such pruning, there is a non zero
  probability that all of the iterations are assigned to one particular worker.
  This loop evaluates the \texttt{where} clauses and distributes the valid iteration
  over workers in a balanced way.
\item \texttt{FragLoopManager} and its subclasses: SIAL supports large sparse
  arrays. To loop over them efficiently SIAL has various implementations of
  fragmented pardo loops. These loops have knowledge of the internal structure of the
  sparse arrays and thus can skip over rows and columns having no useful values.
\end{itemize}
Due to so many varieties of implementation of \texttt{pardo} looping construct
and to support future implementations of indices generation schemes, it is
important to keep the mechanism of prefetching separate from indices generation.
For this, a lazy prefetching mechanism was implemented which will probe for indices
as needed, dynamically. A lazy implementation would also give freedom to vary the number
of prefetched blocks at runtime.

\subsection{Lazy Indices Probing}
Each class implementing \texttt{pardo} have a function \texttt{update} which
calculates the values of the set of indices and populates the interpreter state. This
state is used by interpreter to calculate blocks using an array and index values.

%\hline
\begin{algorithm}  {$update\_indices() \rightarrow bool$}
%\hline %
\singlespacing

\begin{algorithmic}[1]
%\caption{Processing large array in SIAL}\label{alg:euclid}
\Procedure{update\_indices}{}
\ForAll{$indices\ in\ loop$}
\State $old\_index\_val \gets
interpreter\_state.indices[index\_slot]$\Comment{get current index value}
\State $new\_val \gets old\_index\_val + 1$\Comment{Increment the index as needed}
\If{$new\_val \geq upper\_bound[index\_slot]$}
  \State $new\_val \gets lower\_seg[index\_slot]$
\EndIf
\State $interpreter\_state.indices[index\_slot] \gets new\_val$\Comment{update
  the interpreter state}
\EndFor
\If{$all\ indices\ reached\ upper\_bound$}
  \State \Return{false}
\Else
  \State \Return{true}
\EndIf
\EndProcedure
%\hline
\end{algorithmic}
\end{algorithm}

To implement lazy probing, the work done by procedure \texttt{update} is divided
into multiple procedures:

\begin{itemize}
\item \texttt{get\_next\_indices} produces set of \textit{next} indices purely
  based on indices passed as a parameter rather than getting directly from interpreter
  state. This allows us to produce series of indices independent of the state of
  the interpreter.
  \begin{algorithm}  {get\_next\_indices([index]) $\rightarrow$ [index]}
    \singlespacing

    \begin{algorithmic}[1]
      \Function{get\_next\_indices}{$current\_indices$}
      \ForAll{$index\_id\ in\ loop$}
      \State $old\_index\_val \gets current\_indices[index\_id]$\Comment{get current index value}
      \State $new\_val \gets old\_index\_val + 1$\Comment{Increment the index as needed}
      \State $new\_indices \gets new\_val$\Comment{update the index into new set of indices}
      \EndFor%
      \State \Return{$new\_indices$}\Comment{return new set independent of interpreter state}
      \EndFunction
      % \hline
    \end{algorithmic}
  \end{algorithm}

\item \texttt{peek\_indices} returns set of indices and internally takes care of
  maintaining and creating a list of indices \textit{lazily}. It calls the procedure
  \texttt{get\_next\_indices} to produce next set of indices by passing the last
  set of indices in the list as needed. It increases the length of the list by 1 if
  the set of indices requested for is the last one on the list and there are more indices
  in the loop.

  \begin{algorithm}  {peek\_indices(IndexList::iterator) $\rightarrow$ [index], IndexList::iterator}
    \singlespacing

    \begin{algorithmic}[1]
      \Function{peek\_indices}{$it$}
      \If{$IndexList.empty()$}
      \State \Return $[\ ]$
      \Else
      \State $peekedIndices \gets *it$
      \If{$next(it) == IndexList.end()$}
      \State $new\_indices \gets get\_next\_indices(*it)$
      \State $IndexList.insert\_after(it, new\_indices)$
      \EndIf
      \State \Return $peekedIndices, it$
      \EndIf
      \EndFunction
      % \hline
    \end{algorithmic}
  \end{algorithm}

\item \texttt{prefetch\_indices} remember the last set of indices returned to each
  \texttt{GET} statement and returns the next set of indices when it is called. It remembers
  that by mapping the position of indices in the list to \texttt{line numbers} of each
  \texttt{GET}. This makes varying the number of prefetched blocks for each
  \texttt{GET} possible.

  \begin{algorithm} {prefetch\_indices() $\rightarrow$ [index]}
    \singlespacing

    \begin{algorithmic}[1]
      \Function{prefetch\_indices}{}
      \State $line\_number \gets Interpreter.current\_line\_number()$

      \If{$prefetch\_map.contains(line\_number)$}
      \State $it \gets prefetch\_map[line\_number]$
      \Else
      \State $it \gets prefetch\_map.begin()$
      \EndIf
      \State $\{prefetched\_indices, it\} \gets peek\_indices(it)$
      \State $prefetch\_map[line\_number] \gets it$
      \State \Return $prefetch\_indices$
      \EndFunction
    \end{algorithmic}
  \end{algorithm}

  \begin{figure}[h] %place figure "here"
    \centering

    \tikzstyle{table}=[
    matrix of nodes,
    row sep=-\pgflinewidth,
    column sep=-\pgflinewidth,
    nodes={rectangle,draw=black,text width=15ex,align=center},
    text depth=0.25ex,
    text height=1.5ex,
    nodes in empty cells]
    \begin{tikzpicture}[list/.style={rectangle split, rectangle split parts=7,
        draw, rectangle split horizontal}, >=stealth, start chain]

      \node[list,on chain] (A) {};
      \node[list,on chain] (B) {};
      \node[list,on chain] (C) {};
      \node[on chain,draw,inner sep=6pt] (D) {};
      \draw (D.north east) -- (D.south west);
      \draw (D.north west) -- (D.south east);
      \draw[*->] let \p1 = (A.seven), \p2 = (A.center) in (\x1,\y2) -- (B);
      \draw[*->] let \p1 = (B.seven), \p2 = (B.center) in (\x1,\y2) -- (C);
      \draw[*->] let \p1 = (C.seven), \p2 = (C.center) in (\x1,\y2) -- (D);

      \matrix[table,below=of A] (map)
      {
        {Line Number}  & {Reference}  \\
        {101}          & {}           \\
        {102}          & {}           \\
        {\vdots}       & {\vdots}     \\
      };

      \draw[*->] (map-2-2.center) to[out=0, in=270] (B.south);
      \draw[*->] (map-3-2.center) to[out=0, in=270] (C.south);
    \end{tikzpicture}
    \caption{\texttt{prefetch\_indices} saves mapping of line number to position in the list} \label{fig:prefetch_indices map}
  \end{figure}

\item \texttt{update} is now changed to simply pop the first set of indices from
  the list and update interpreter state so that other modules can find the value
  of current indices. This decreases the length of the list by 1.
  \begin{algorithm}  {update\_indices() $\rightarrow$ bool}
    \singlespacing

    \begin{algorithmic}[1]
      \Function{update\_indices}{}
      \State $current\_indices \gets peek\_indices(IndexList.begin())$
      \If{$length(current\_indices) > 0$}\Comment{is there any more iteration}
      \State $Indexlist.pop()$
      \ForAll{$index\_slot\ in\ current\_indices $}
      \State $interpreter\_state.indices[index\_slot] \gets current\_indices[index\_slot]$
      \EndFor
      \State \Return $true$
      \Else
      \State \Return $false$
      \EndIf
      \EndFunction
      % \hline
    \end{algorithmic}
  \end{algorithm}
\end{itemize}

In all, the functions \texttt{peek\_indices} and \texttt{update} can be modeled
as producer and consumer problem on a bounded buffer. And function \texttt{prefetch}
indices is free to point at any set of indices on the list.

  \begin{figure}[h] %place figure "here"
    \centering
    \begin{tikzpicture}[
      list/.style={rectangle split, rectangle split parts=7,
        draw, rectangle split horizontal},
      function/.style={rectangle, draw,align=center},
      >=stealth, start chain]

      \node[list,on chain] (A) {};
      \node[list,on chain] (B) {};
      \node[list,on chain] (C) {};
      \node[on chain,draw,inner sep=6pt] (D) {};
      \draw (D.north east) -- (D.south west);
      \draw (D.north west) -- (D.south east);
      \draw[*->] let \p1 = (A.seven), \p2 = (A.center) in (\x1,\y2) -- (B);
      \draw[*->] let \p1 = (B.seven), \p2 = (B.center) in (\x1,\y2) -- (C);
      \draw[*->] let \p1 = (C.seven), \p2 = (C.center) in (\x1,\y2) -- (D);

      \node[function, below=of A]        (update)   {\texttt{update}};
      \node[function, right=of update]   (prefetch) {\texttt{prefetch\_indices}};
      \node[function, right=of prefetch] (peek)     {\texttt{peek\_indices}};

      \draw[->]                (update.north)   -- (A.south);
      \draw[densely dotted,->] (prefetch.north) -- (A.south);
      \draw[densely dotted,->] (prefetch.north) -- (B.south);
      \draw[densely dotted,->] (prefetch.north) -- (C.south);
      \draw[->]                (peek.north)     -- (C.south);
    \end{tikzpicture}
    \caption{\texttt{update} consumes, \texttt{peek\_indices} produces if needed, \texttt{prefetch\_indices} is free to move along list} \label{fig:producer-consumer}
  \end{figure}
