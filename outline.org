#+TITLE: Enhancements to Looping Constructs By Prefetching and Exploiting GPU in SIA
#+AUTHOR: Anurag Peshne

* Introduction
** Introduction to Issues with Working with GPU
   In the previous version of ACES, the SIAL programmer had to manage the memory
   transfer between GPU and CPU. While in ACES4 this has been automated by tracking
   block changes using version numbers, there is still need to programmer to mark
   regions of code to be executed on GPU.

   Transfering data between GPU and CPU is expensive. And this can make executing a
   calculation on GPU more expensive than on CPU. Judging the target computational
   device is difficult since the time required depends on size of block. Larger
   the block, longer it takes for transfering but at same time difference in
   execution time on GPU and CPU increases as block size increases. Also, since
   same instructions are used to calculate with varying data size, same instruction
   can be more expensive on GPU or CPU depending on size of data provided.
   Various GPU techniques are implemented to solve this problem and to improve
   transfer time and to effectively use GPUs to execute looping constructs.
** Introduction to Issue of Data Transfer from Server
   Looping constructs are the only way to work with Blocks. Typical workflow of
   SIAL includes requesting block of larger array at start of the loop, processing
   the block and then sending the block back to the server. The request to server
   for block is blocking. To mitigate the cost of sending data over network, prefetching
   is implemented which does the transfer concurrently with the block processing.
** TODO Organization of Thesis
* Related Work
** GPGPU
*** Directive Based GPU Programming
*** GPU in Computational Chemistry
*** GPU Programming in Other High Level Programming Languages
*** GPU in Previous Version of ACES
** Prefetching
   - Prefetching in hardware and compiler.
   - hardware prefetching includes which memory address to prefetch based on past
     access or future instructions.
     - ~fetch~ instruction simply passes the address on to memory system without
       forcing the processor to wait for a response. Similar to ~load~ except the
       word is not forwarded to the processor after it has been cached.
     - prematurely prefetched block may also displace data in the cache that is
       currently in use by the processor resulting in what is known as
       /cache pollution/
     - Although uneccessary prefetches do not affect correct program behavior,
       they consume memory bandwidth
     - Prefetching is not restricted to fetching data from main memory into
       processor cache. Rather it is a generally applicable technique for moving
       memory objects up in the memory hierarchy before they are actually needed
       by processor.
     - Adaptive sequential prefetching policy that allows the value of K to vary
       during program execution in such way to match K to degree of spatial locality
       exhibited by program at a particular time.
     - optimize loop, vector operation by guessing upcoming access and prefetch.
   - compiler issues prefetch instructions
     - DASH paper
     - DAE: decoupled access execute
   - Similarities and difference
     - In SIAL, we know which blocks need to be prefetched.
     - know the exact sequence of blocks needed.
     - where clauses can be evaluated during indices probing to know which iteration
       will be evaluated
     - similar as blocks are requested before they are needed to reduce/eliminate
       block wait time.
     - prefetching can be clubbed with parallel IO to get more throughput.
     - complex cache algorithms not required since blocks will not be used again
       in a loop, once used - can safely evict from cache/prefetched memory.
* Background
** SIA
** Architecture of SIA
** SIAL
*** SIAL Interpreter
** Block Structure
** Executing Super Instructions on GPU
** Overview of ACES
* Block Prefetching
** Background
** Implementation of Prefetching
*** ~pardo~ Loop Implementation
** Lazy Indices Probing
* GPU Techniques
** Memory Pinning
*** ~memcpy~ without Pinning
*** TODO Reuse allocated blocks
** CUDA aware MPI
** Streams
*** Non Blocking Copying
** TODO MPS
* TODO Experiment and Results
* Conclusion and Future Work
** Make use of Non blocking copying
** Exploit multiple ~GET~ lookahead
** Dynamic adjustment of K, similar to adaptive sequential prefetching policy.
