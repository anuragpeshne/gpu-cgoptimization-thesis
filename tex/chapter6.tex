\chapter{CONCLUSION AND FUTURE WORK}\label{conclusion}
Two approaches to enhance the looping constructs and improve the runtime efficiency
of SIAL interpreter are presented in this thesis. One applies the technique of
data block prefetching to SIA. The other improves
the utilization of GPU to execute Super Instructions on GPU and make way to support
other compute devices in future.

Though prefetching makes subsequent requests to a server efficient by reducing or
eliminating \texttt{wait\_time}, it makes the first request to the server expensive.
Thus to make up for it, the length of the range of indices should be long enough. Hit
ratio, as defined in section~\ref{sec:hit_ratio}, can be used as a metric to evaluate
the performance of prefetching. As seen in section~\ref{sec:bp_molecule}, prefetching
helps in reducing overall \texttt{wait\_time} when most of the \texttt{wait\_time}
is due to blocking for data block transfer over the network. Also, the total number
of workers participating in the computation should be taken in consideration.
If the number of workers is very high then the index range will be divided among high number
of workers and each individual worker will not get sufficient index range for prefetching.

It was found that the optimal number of blocks to prefetch in most cases is one.
This result is not sensitive to the block size.

Executing super instructions on GPU can speed up the computation when block sizes
are large enough. Transferring blocks between a GPU and host memory is expensive but it can be
avoided or reduced by directly transferring blocks from the GPU memory.
Page locked memory
can improve memory transfer speed by invoking DMA and bypassing CPU. But allocating page locked
memory is expensive as compared to dynamically allocated memory. Page locked
memory blocks can be cached and served with high cache hit ratio.

Several further improvements to GPU utilization in ACES4 can be envisioned. First,
more super instructions on GPUs should be implemented along with finding a way to
easily port user defined super instructions to GPUs. Another optimization would be
to exploit the asynchronous memory synchronization between the GPU memory
and the host memory by looking ahead for instructions which need the memory
to be transferred and initiating the asynchronous transfer as soon as data is ready.
Feasibility of having access to GPUs on servers and implementing
RDMA to transfer memory blocks between workers and servers can also be explored. Lastly, implementing
a more advanced caching mechanism for page locked memory blocks in order to have a
more efficient caching mechanism.
